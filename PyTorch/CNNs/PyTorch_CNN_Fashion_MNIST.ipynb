{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PyTorch CNN Fashion MNIST.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8wOnxPbdYl9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvJF-LtfdiVZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b3e3d14b-5dd0-4618-b752-f29f49f9389c"
      },
      "source": [
        "train_dataset = torchvision.datasets.FashionMNIST(\n",
        "    '.',\n",
        "    train=True,\n",
        "    transform = transforms.ToTensor(),\n",
        "    download = True\n",
        ")\n",
        "test_dataset = torchvision.datasets.FashionMNIST(\n",
        "    '.',\n",
        "    train=False,\n",
        "    transform = transforms.ToTensor(),\n",
        "    download = True\n",
        ")\n",
        "# number of classes\n",
        "K = len(set(train_dataset.targets.numpy()))\n",
        "print(\"We have %s classes for this problem\" % K)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We have 10 classes for this problem\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RfuXy0reToc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define the model\n",
        "class CNN(nn.Module):\n",
        "  def __init__(self, K):\n",
        "    super(CNN, self).__init__()\n",
        "    # conv layers\n",
        "    self.conv_layers = nn.Sequential(\n",
        "      # 28x28x1 -> 13x13x32\n",
        "      nn.Conv2d(in_channels = 1, out_channels = 32, kernel_size = 3, stride = 2),\n",
        "      nn.ReLU(),\n",
        "      # 13x13 -> 6x6x64\n",
        "      nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size = 3, stride = 2),\n",
        "      nn.ReLU(),\n",
        "      # 6x6 -> 2x2x128 (HxWxC) where C is the feature maps.\n",
        "      nn.Conv2d(in_channels = 64, out_channels = 128, kernel_size = 3, stride = 2,),\n",
        "      nn.ReLU()\n",
        "    )\n",
        "    # dense layers\n",
        "    self.dense_layers = nn.Sequential(\n",
        "      nn.Dropout(.2), # drop 20% of the nodes randomly\n",
        "      nn.Linear(128 * 2 * 2, 512), # calculate output of conv layers and set as input\n",
        "      nn.ReLU(),\n",
        "      nn.Dropout(.2),\n",
        "      nn.Linear(512, K)   \n",
        "    )\n",
        "  \n",
        "  def forward(self, x):\n",
        "    out = self.conv_layers(x)\n",
        "    out = out.view(out.size(0), -1)\n",
        "    out = self.dense_layers(out)\n",
        "    return out\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfR2L4-tmLq0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# initalize the model\n",
        "cnn_model = CNN(K)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qoVrMfn8mWzL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "3afef7e4-8da3-49af-abca-dfdafb6d179f"
      },
      "source": [
        "# pick a device\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print('selected %s ' % device)\n",
        "# pass the model to selected device\n",
        "cnn_model.to(device)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "selected cuda:0 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CNN(\n",
              "  (conv_layers): Sequential(\n",
              "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2))\n",
              "    (1): ReLU()\n",
              "    (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2))\n",
              "    (3): ReLU()\n",
              "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2))\n",
              "    (5): ReLU()\n",
              "  )\n",
              "  (dense_layers): Sequential(\n",
              "    (0): Dropout(p=0.2, inplace=False)\n",
              "    (1): Linear(in_features=512, out_features=512, bias=True)\n",
              "    (2): ReLU()\n",
              "    (3): Dropout(p=0.2, inplace=False)\n",
              "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3pvHsOnmjNK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define the loss and optimizers\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(cnn_model.parameters())"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ln5msGmm21O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create data loaders for given train and test sets\n",
        "batch_size = 128\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size = batch_size, \n",
        "    shuffle = True\n",
        ")\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size = batch_size,\n",
        "    shuffle = False\n",
        ")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmRT1N5unJum",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define prediction and training functions\n",
        "def batch_gd(model, criterion, optimizer, train_loader, test_loader, epochs):\n",
        "  \"\"\"\n",
        "  Apply batch gradient descent to given model and data.\n",
        "  Data must be DataLoader object\n",
        "  \"\"\"\n",
        "  \n",
        "  train_losses = np.zeros(epochs)\n",
        "  test_losses = np.zeros(epochs)\n",
        "\n",
        "  for i in range(epochs):\n",
        "    t0 = time.time()\n",
        "    train_loss = []\n",
        "    test_loss = []\n",
        "    for inputs, targets in train_loader:\n",
        "      inputs, targets = inputs.to(device), targets.to(device)\n",
        "      # we don't need to flatten the input since\n",
        "      # we use CNN's as first layers of our NN\n",
        "\n",
        "      # forward pass\n",
        "      outputs = model(inputs)\n",
        "      loss = criterion(outputs, targets)\n",
        "      # backward propagation\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()\n",
        "      # keep the loss\n",
        "      train_loss.append(loss.item())\n",
        "\n",
        "    # calculate the loss for training too\n",
        "    for inputs, targets in test_loader:\n",
        "      inputs, targets = inputs.to(device), targets.to(device)\n",
        "      outputs = model(inputs)\n",
        "      loss = criterion(outputs, targets)\n",
        "      test_loss.append(loss.item())\n",
        "    \n",
        "    # get the average loss for both train and test sets and keep them too\n",
        "    avg_train_loss = np.mean(train_loss)\n",
        "    avg_test_loss = np.mean(test_loss)\n",
        "    train_losses[i] = avg_train_loss\n",
        "    test_losses[i] = avg_test_loss\n",
        "\n",
        "    # calculate the time\n",
        "    t1 = time.time()\n",
        "\n",
        "    print(\"-\"*50)\n",
        "    print(f\"Epoch {i+1}/{epochs} took {t1-t0:.3f}s \\n avg_loss for train: {avg_train_loss:.4f} \\n avg_loss for test: {avg_test_loss:.4f}\")\n",
        "    print(\"-\"*50)\n",
        "\n",
        "  return train_loss, test_loss\n",
        "\n",
        "def validation_predict(model, data_loader, acc=True):\n",
        "  \"\"\"\n",
        "  Make predictions with given DataLoader object. \n",
        "  Return the predictions and accuracy(if acc=True) \n",
        "  \"\"\"\n",
        "  n_correct = 0\n",
        "  n_total = 0\n",
        "  predictions = np.array([])\n",
        "  with torch.no_grad():\n",
        "    for inputs, targets in data_loader:\n",
        "      # we're getting logits from model, so applying\n",
        "      # softmax would be enough\n",
        "\n",
        "      # pass the data to gpu\n",
        "      inputs, targets = inputs.to(device), targets.to(device)\n",
        "      outputs = model(inputs)\n",
        "\n",
        "      # return the index that has maximum value of a column\n",
        "      # this is almost same as applying softmax itself in our situation\n",
        "      _, preds = torch.max(outputs, 1)\n",
        "      if acc:\n",
        "        accuracy = (preds == targets).sum().item()\n",
        "        n_correct += accuracy\n",
        "        n_total += (targets.shape[0])\n",
        "      predictions = np.concatenate((predictions, preds.cpu().numpy()))\n",
        "\n",
        "  return [predictions,(n_correct/n_total)] if acc else predictions"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofsZ_r4l2tOA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5cf0dae9-4764-4405-c1dd-1730a3435e52"
      },
      "source": [
        "n_epochs = 15\n",
        "train_loss, test_loss = batch_gd(cnn_model, criterion, optimizer, train_loader, test_loader, n_epochs)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--------------------------------------------------\n",
            "Epoch 1/15 took 11.408s \n",
            " avg_loss for train: 0.6472 \n",
            " avg_loss for test: 0.4978\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Epoch 2/15 took 11.210s \n",
            " avg_loss for train: 0.4289 \n",
            " avg_loss for test: 0.4121\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Epoch 3/15 took 11.421s \n",
            " avg_loss for train: 0.3745 \n",
            " avg_loss for test: 0.4011\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Epoch 4/15 took 11.451s \n",
            " avg_loss for train: 0.3382 \n",
            " avg_loss for test: 0.3572\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Epoch 5/15 took 11.419s \n",
            " avg_loss for train: 0.3109 \n",
            " avg_loss for test: 0.3396\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Epoch 6/15 took 11.700s \n",
            " avg_loss for train: 0.2893 \n",
            " avg_loss for test: 0.3428\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Epoch 7/15 took 11.226s \n",
            " avg_loss for train: 0.2677 \n",
            " avg_loss for test: 0.3153\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Epoch 8/15 took 11.028s \n",
            " avg_loss for train: 0.2499 \n",
            " avg_loss for test: 0.3308\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Epoch 9/15 took 11.460s \n",
            " avg_loss for train: 0.2368 \n",
            " avg_loss for test: 0.3067\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Epoch 10/15 took 11.408s \n",
            " avg_loss for train: 0.2208 \n",
            " avg_loss for test: 0.3201\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Epoch 11/15 took 11.136s \n",
            " avg_loss for train: 0.2089 \n",
            " avg_loss for test: 0.3114\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Epoch 12/15 took 11.531s \n",
            " avg_loss for train: 0.1963 \n",
            " avg_loss for test: 0.3035\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Epoch 13/15 took 11.295s \n",
            " avg_loss for train: 0.1857 \n",
            " avg_loss for test: 0.3313\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Epoch 14/15 took 11.359s \n",
            " avg_loss for train: 0.1734 \n",
            " avg_loss for test: 0.3188\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Epoch 15/15 took 11.606s \n",
            " avg_loss for train: 0.1616 \n",
            " avg_loss for test: 0.3446\n",
            "--------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oT85-1XZ331e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6829aa70-abd2-4d2e-f6b1-1f1536fb261b"
      },
      "source": [
        "# calculate the accuracy for train and test sets\n",
        "_, train_accuracy = validation_predict(cnn_model, train_loader)\n",
        "test_preds, test_accuracy = validation_predict(cnn_model, test_loader)\n",
        "print(f\"Train accuracy: {train_accuracy:.4f} | Test accuracy: {test_accuracy:.4f} \")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train accuracy: 0.9410 | Test accuracy: 0.8938 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WOnDRjE6XST",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "outputId": "e12339e0-5b4a-4e13-bd44-14bbc58778e0"
      },
      "source": [
        "# now plot the losses again with plotly\n",
        "import plotly.graph_objects as go\n",
        "fig = go.Figure()\n",
        "fig.add_trace(go.Scatter(x=list(range(1,n_epochs+1)), y=train_loss, name='Train loss', mode = 'lines+markers'))\n",
        "fig.add_trace(go.Scatter(x=list(range(1,n_epochs+1)), y=test_loss, name='Test loss', mode = 'lines+markers'))\n",
        "fig.show()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"86e4eb06-d38c-4b22-a0d6-13a437efac37\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"86e4eb06-d38c-4b22-a0d6-13a437efac37\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '86e4eb06-d38c-4b22-a0d6-13a437efac37',\n",
              "                        [{\"mode\": \"lines+markers\", \"name\": \"Train loss\", \"type\": \"scatter\", \"x\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], \"y\": [0.15841445326805115, 0.09503093361854553, 0.10487008094787598, 0.168836772441864, 0.16100896894931793, 0.1529899537563324, 0.1400991827249527, 0.2014206498861313, 0.20957553386688232, 0.18449266254901886, 0.13588887453079224, 0.09210444241762161, 0.1098887249827385, 0.11395431309938431, 0.20522430539131165, 0.14175410568714142, 0.12199670076370239, 0.1443023830652237, 0.16654404997825623, 0.16147437691688538, 0.13954365253448486, 0.2773723006248474, 0.10439901053905487, 0.14712992310523987, 0.13718318939208984, 0.13648228347301483, 0.11316931247711182, 0.17987093329429626, 0.15902109444141388, 0.12774041295051575, 0.08755149692296982, 0.15938523411750793, 0.05698402598500252, 0.09219266474246979, 0.194495290517807, 0.06996052712202072, 0.1646626591682434, 0.12441231310367584, 0.08938177675008774, 0.18003976345062256, 0.14191824197769165, 0.09807336330413818, 0.0915171205997467, 0.1313234269618988, 0.17933695018291473, 0.20981372892856598, 0.12597179412841797, 0.1125098168849945, 0.13609059154987335, 0.13450896739959717, 0.25376251339912415, 0.22146490216255188, 0.12027963995933533, 0.14251844584941864, 0.14964550733566284, 0.15039008855819702, 0.13109387457370758, 0.17724934220314026, 0.1426105946302414, 0.10820085555315018, 0.13375671207904816, 0.2131655514240265, 0.20382973551750183, 0.12109032273292542, 0.20795458555221558, 0.19768142700195312, 0.0853268951177597, 0.12530703842639923, 0.16660019755363464, 0.14744386076927185, 0.2154209166765213, 0.17433783411979675, 0.21529002487659454, 0.14420783519744873, 0.13655400276184082, 0.14158041775226593, 0.21117877960205078, 0.16591423749923706, 0.13367947936058044, 0.18161886930465698, 0.16254167258739471, 0.2090316265821457, 0.09591442346572876, 0.12594187259674072, 0.14386527240276337, 0.15627293288707733, 0.18209734559059143, 0.23868265748023987, 0.10810834169387817, 0.21725164353847504, 0.10502756386995316, 0.12109056860208511, 0.14625318348407745, 0.15028923749923706, 0.1645612120628357, 0.15752515196800232, 0.11477768421173096, 0.1264985352754593, 0.1133304312825203, 0.20900973677635193, 0.17875920236110687, 0.0773700475692749, 0.24184860289096832, 0.27157387137413025, 0.23784135282039642, 0.08395212888717651, 0.20398499071598053, 0.10246449708938599, 0.08729971200227737, 0.1066637635231018, 0.22888199985027313, 0.16059371829032898, 0.137040376663208, 0.11311663687229156, 0.19302842020988464, 0.1676369607448578, 0.12217219173908234, 0.1503535658121109, 0.24409691989421844, 0.12915298342704773, 0.1344352662563324, 0.16291488707065582, 0.11900585889816284, 0.1704389899969101, 0.20742541551589966, 0.14500349760055542, 0.12542426586151123, 0.15046213567256927, 0.16246765851974487, 0.16342639923095703, 0.22551092505455017, 0.15517906844615936, 0.3430710732936859, 0.16958411037921906, 0.2765735685825348, 0.07168425619602203, 0.0958847627043724, 0.1685238778591156, 0.14204201102256775, 0.071200892329216, 0.14457246661186218, 0.15189406275749207, 0.12203986942768097, 0.16903658211231232, 0.14233176410198212, 0.1464722603559494, 0.1511038839817047, 0.10230313241481781, 0.14646217226982117, 0.175351083278656, 0.10385262966156006, 0.1551591157913208, 0.26785382628440857, 0.2029390037059784, 0.19335614144802094, 0.09838028252124786, 0.17255115509033203, 0.12377792596817017, 0.12052561342716217, 0.29990214109420776, 0.15913507342338562, 0.23847024142742157, 0.20656228065490723, 0.1047971099615097, 0.2797357141971588, 0.16840311884880066, 0.12576265633106232, 0.10574572533369064, 0.16264469921588898, 0.11523719877004623, 0.17703020572662354, 0.1833985149860382, 0.19167771935462952, 0.20081236958503723, 0.14404579997062683, 0.1527687907218933, 0.17876827716827393, 0.2114584743976593, 0.20017722249031067, 0.15047003328800201, 0.19992467761039734, 0.21115899085998535, 0.15501251816749573, 0.08075796067714691, 0.12398304045200348, 0.14815619587898254, 0.22619163990020752, 0.18489830195903778, 0.26154956221580505, 0.17360998690128326, 0.20408226549625397, 0.24001440405845642, 0.22470973432064056, 0.15661941468715668, 0.16580486297607422, 0.13301807641983032, 0.16919207572937012, 0.12606127560138702, 0.23316408693790436, 0.1700206995010376, 0.08273540437221527, 0.14408712089061737, 0.22795040905475616, 0.11541838943958282, 0.15213477611541748, 0.18454855680465698, 0.17467866837978363, 0.13904644548892975, 0.16990451514720917, 0.1761155128479004, 0.10188229382038116, 0.19336473941802979, 0.15382428467273712, 0.23413291573524475, 0.15515708923339844, 0.23511673510074615, 0.15886381268501282, 0.28010252118110657, 0.24110502004623413, 0.2443787306547165, 0.16124236583709717, 0.1808444857597351, 0.09580928832292557, 0.16329365968704224, 0.11334028840065002, 0.12537305057048798, 0.19861319661140442, 0.2705838978290558, 0.1584537774324417, 0.1619521826505661, 0.21252110600471497, 0.09074938297271729, 0.1594737470149994, 0.11055544018745422, 0.18414995074272156, 0.16363999247550964, 0.16414009034633636, 0.18219590187072754, 0.16685715317726135, 0.08779771625995636, 0.1708061397075653, 0.12155262380838394, 0.17506538331508636, 0.16802173852920532, 0.1479721963405609, 0.16315634548664093, 0.2036035656929016, 0.16257540881633759, 0.15281160175800323, 0.19389072060585022, 0.13246652483940125, 0.19599314033985138, 0.21381360292434692, 0.20483152568340302, 0.20949295163154602, 0.20420828461647034, 0.10027532279491425, 0.09455408900976181, 0.132995143532753, 0.09795349091291428, 0.17572623491287231, 0.20184245705604553, 0.13017188012599945, 0.1605384349822998, 0.20004288852214813, 0.11311065405607224, 0.26026099920272827, 0.19748947024345398, 0.08067250996828079, 0.2098264992237091, 0.1747303009033203, 0.18057209253311157, 0.15648427605628967, 0.1796983927488327, 0.07836253196001053, 0.1385582834482193, 0.1807166188955307, 0.17481473088264465, 0.2006160318851471, 0.09290696680545807, 0.1791117638349533, 0.11736956238746643, 0.1672484278678894, 0.17603400349617004, 0.1604805588722229, 0.1337331235408783, 0.12473222613334656, 0.0992467850446701, 0.2465364784002304, 0.16873857378959656, 0.1232241839170456, 0.1280524879693985, 0.2573500871658325, 0.2080717384815216, 0.19663165509700775, 0.1837121695280075, 0.17394915223121643, 0.2153109461069107, 0.19579166173934937, 0.17754070460796356, 0.12030613422393799, 0.2120581865310669, 0.12931734323501587, 0.1882046014070511, 0.15529555082321167, 0.14441145956516266, 0.14302316308021545, 0.12543044984340668, 0.1318555474281311, 0.25945115089416504, 0.14129476249217987, 0.0977034941315651, 0.13867859542369843, 0.1281106323003769, 0.11775776743888855, 0.13151980936527252, 0.17311221361160278, 0.20539021492004395, 0.22921159863471985, 0.1576240360736847, 0.23608852922916412, 0.11267402768135071, 0.1593823879957199, 0.20316006243228912, 0.13132207095623016, 0.07559573650360107, 0.12335838377475739, 0.1148390918970108, 0.22996802628040314, 0.23837099969387054, 0.20876799523830414, 0.11955868452787399, 0.1853993535041809, 0.14690741896629333, 0.19369329512119293, 0.16929832100868225, 0.16325590014457703, 0.14244934916496277, 0.10982241481542587, 0.11233238875865936, 0.1694338470697403, 0.10218612104654312, 0.1354944407939911, 0.22267594933509827, 0.17729178071022034, 0.1577782928943634, 0.1171879917383194, 0.13135769963264465, 0.22381563484668732, 0.12766464054584503, 0.14789599180221558, 0.1860615611076355, 0.1927190124988556, 0.12855064868927002, 0.15382105112075806, 0.2087557464838028, 0.14377513527870178, 0.1974502056837082, 0.21561241149902344, 0.16397841274738312, 0.11126603186130524, 0.22942744195461273, 0.183222234249115, 0.17625951766967773, 0.10921057313680649, 0.1540018916130066, 0.3298160433769226, 0.15403780341148376, 0.15446574985980988, 0.22955171763896942, 0.1398550271987915, 0.15666979551315308, 0.18475483357906342, 0.1087149828672409, 0.24568192660808563, 0.2320355325937271, 0.19279047846794128, 0.18177422881126404, 0.11198587715625763, 0.1939568966627121, 0.21460482478141785, 0.17636263370513916, 0.16838102042675018, 0.14159169793128967, 0.11900171637535095, 0.22310450673103333, 0.1153554618358612, 0.19224607944488525, 0.12728816270828247, 0.23560123145580292, 0.15766054391860962, 0.13046479225158691, 0.18799825012683868, 0.13469406962394714, 0.1513574868440628, 0.15767544507980347, 0.0994468629360199, 0.14104212820529938, 0.09600065648555756, 0.22747020423412323, 0.10510589182376862, 0.20025315880775452, 0.13802629709243774, 0.1328289657831192, 0.17640990018844604, 0.1961439847946167, 0.20972712337970734, 0.22707217931747437, 0.1660621464252472, 0.16817601025104523, 0.16201342642307281, 0.2050638347864151, 0.16634619235992432, 0.1346825659275055, 0.18875057995319366, 0.16069504618644714, 0.1421232670545578, 0.1734972596168518, 0.1930893063545227, 0.17157131433486938, 0.14534536004066467, 0.17196084558963776, 0.16724728047847748, 0.30853620171546936, 0.16926053166389465, 0.2234644740819931, 0.09807814657688141, 0.13691487908363342, 0.23400180041790009, 0.16395303606987, 0.1386810541152954, 0.13012826442718506, 0.15569348633289337, 0.21160905063152313, 0.23843874037265778, 0.17147953808307648, 0.09547235816717148, 0.14555224776268005, 0.1499543935060501, 0.15871302783489227, 0.14317746460437775, 0.13255217671394348, 0.09744799882173538, 0.1354251205921173, 0.23295237123966217, 0.12357836961746216, 0.1753794550895691, 0.07787803560495377, 0.14212965965270996, 0.20614434778690338, 0.09582063555717468, 0.12008732557296753, 0.11349761486053467, 0.07946643233299255, 0.1379733681678772, 0.12593014538288116, 0.09482382237911224, 0.11970742791891098, 0.19145219027996063, 0.1944282501935959, 0.12312494963407516, 0.15423598885536194, 0.14427755773067474, 0.10511894524097443, 0.14372481405735016, 0.1525823026895523, 0.14164161682128906, 0.23533043265342712, 0.16579025983810425]}, {\"mode\": \"lines+markers\", \"name\": \"Test loss\", \"type\": \"scatter\", \"x\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], \"y\": [0.42205721139907837, 0.31655803322792053, 0.22795900702476501, 0.3208080232143402, 0.21619084477424622, 0.5847133994102478, 0.3546837866306305, 0.5416692495346069, 0.31169813871383667, 0.3413059711456299, 0.3737870156764984, 0.2603954076766968, 0.2354418784379959, 0.21093358099460602, 0.2295060157775879, 0.4273354411125183, 0.2706655263900757, 0.3164295256137848, 0.2969813942909241, 0.3078311085700989, 0.43333670496940613, 0.32839399576187134, 0.7152527570724487, 0.45129892230033875, 0.34029123187065125, 0.5156196355819702, 0.39343446493148804, 0.5850321650505066, 0.29466283321380615, 0.4037649631500244, 0.3867402672767639, 0.2938864827156067, 0.40058010816574097, 0.24753710627555847, 0.23328205943107605, 0.31884703040122986, 0.49808767437934875, 0.4378460645675659, 0.3955700695514679, 0.45487290620803833, 0.27913588285446167, 0.3296048045158386, 0.43999144434928894, 0.5493756532669067, 0.2939525246620178, 0.2574048638343811, 0.4410163462162018, 0.30835598707199097, 0.28881192207336426, 0.30210214853286743, 0.35146284103393555, 0.3784748911857605, 0.3955131769180298, 0.2869917154312134, 0.3219909071922302, 0.4177137613296509, 0.304572194814682, 0.2610129415988922, 0.13963060081005096, 0.23988434672355652, 0.20919765532016754, 0.4223472476005554, 0.38055160641670227, 0.24580474197864532, 0.23944362998008728, 0.19000181555747986, 0.31354397535324097, 0.2521917223930359, 0.40848618745803833, 0.5314586758613586, 0.32658347487449646, 0.5388720035552979, 0.42709359526634216, 0.27161532640457153, 0.23342350125312805, 0.37506577372550964, 0.24296578764915466, 0.29130491614341736, 0.03753955662250519]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('86e4eb06-d38c-4b22-a0d6-13a437efac37');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8F4nplSN8Rkg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# label mapping to see misclassified predictions\n",
        "labels = '''T-shirt/top\n",
        "Trouser\n",
        "Pullover\n",
        "Dress\n",
        "Coat\n",
        "Sandal\n",
        "Shirt\n",
        "Sneaker\n",
        "Bag\n",
        "Ankle boot\n",
        "'''.split(\"\\n\")"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgiQSNhx85O-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "d072731c-50ea-4c44-ff14-d3b37c6249ab"
      },
      "source": [
        "test_preds = test_preds.astype(np.uint8)\n",
        "test_inputs = test_dataset.data.numpy()\n",
        "test_targets = test_dataset.targets.numpy()\n",
        "misclassified_idx = np.where(test_preds != test_targets)[0]\n",
        "print(f\"Total misclassified image count is {len(misclassified_idx)}\")\n",
        "print(f\"{(len(misclassified_idx)/test_targets.shape[0]) * 100:.2f}% of the images are misclassified by the model.\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total misclassified image count is 1062\n",
            "10.62% of the images are misclassified by the model.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GuMaTSvp_r9V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "11c7a5be-734e-49c2-da03-5c83fb3914ca"
      },
      "source": [
        "# plot a random example\n",
        "r = np.random.choice(misclassified_idx)\n",
        "plt.imshow(test_inputs[r].reshape(28,28), cmap = 'gray')\n",
        "plt.title(f\"True label: {labels[test_targets[r]]} | Predicted: {labels[test_preds[r]]}\")\n",
        "plt.show()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}