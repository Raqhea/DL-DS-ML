{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Fashion MNIST Image Classification TF2.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPn0+hkMLLUykhSzvitSrCq"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"-s5IyXSCqkJM","colab_type":"code","outputId":"b8975f26-2bc1-43d0-b0d8-422a004fedc3","executionInfo":{"status":"ok","timestamp":1582623048534,"user_tz":-180,"elapsed":7534,"user":{"displayName":"Alper Balbay","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDfSH89H_io39Rxl1rTT_RIw29YUrP-dmPlq0Xi=s64","userId":"00418095213724128690"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["%tensorflow_version 2.x\n","import tensorflow as tf\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","# dataset\n","from tensorflow.keras.datasets import fashion_mnist"],"execution_count":1,"outputs":[{"output_type":"stream","text":["TensorFlow 2.x selected.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"twZfPjmgqx1R","colab_type":"code","outputId":"e27f8375-e9f0-4327-e2f6-3db69cff51ec","executionInfo":{"status":"ok","timestamp":1582623048931,"user_tz":-180,"elapsed":7897,"user":{"displayName":"Alper Balbay","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDfSH89H_io39Rxl1rTT_RIw29YUrP-dmPlq0Xi=s64","userId":"00418095213724128690"}},"colab":{"base_uri":"https://localhost:8080/","height":174}},"source":["# loading dataset\n","fashion = fashion_mnist.load_data()"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n","32768/29515 [=================================] - 0s 0us/step\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n","26427392/26421880 [==============================] - 0s 0us/step\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n","8192/5148 [===============================================] - 0s 0us/step\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n","4423680/4422102 [==============================] - 0s 0us/step\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zNlZZkZjq_R1","colab_type":"code","outputId":"67a2108c-b219-4e16-f9e4-1543edbd8c50","executionInfo":{"status":"ok","timestamp":1582623049402,"user_tz":-180,"elapsed":8340,"user":{"displayName":"Alper Balbay","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDfSH89H_io39Rxl1rTT_RIw29YUrP-dmPlq0Xi=s64","userId":"00418095213724128690"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["(x_train, y_train), (x_test, y_test) = fashion\n","# I got the description of the data from : https://www.kaggle.com/zalando-research/fashionmnist\n","# it says every pixel value has a range between 0,255\n","# but i need to reduce that range.\n","# If I divide every pixel value by 255, the range of the pixel values will reduce to 0,1\n","x_train, x_test = x_train/255, x_test/255\n","print('x_train.shape:', x_train.shape)\n","print('x_test.shape:', x_test.shape)\n","print('y_train.shape:', y_train.shape)\n","print('y_test.shape:', y_test.shape)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["x_train.shape: (60000, 28, 28)\n","x_test.shape: (10000, 28, 28)\n","y_train.shape: (60000,)\n","y_test.shape: (10000,)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zQP6-JfvtEQ-","colab_type":"code","outputId":"d58f4f0f-7b63-459e-dbc9-491b03992e85","executionInfo":{"status":"ok","timestamp":1582533043637,"user_tz":-180,"elapsed":10199,"user":{"displayName":"Alper Balbay","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDfSH89H_io39Rxl1rTT_RIw29YUrP-dmPlq0Xi=s64","userId":"00418095213724128690"}},"colab":{"base_uri":"https://localhost:8080/","height":705}},"source":["# Now I can create the model\n","# Actually i'd need to flatten the data but there's a layer for it, so i dont need to do it manually\n","\n","model = tf.keras.models.Sequential([\n","  tf.keras.layers.Flatten(input_shape = (28,28)),\n","  tf.keras.layers.Dense(256, activation = 'relu'),\n","  tf.keras.layers.Dropout(.25),\n","  tf.keras.layers.Dense(10, activation = 'softmax')\n","])\n","\n","model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n","\n","r = model.fit(x_train, y_train, validation_data = (x_test, y_test), epochs = 50)\n","\n","# plot the results\n","plt.plot(r.history['loss'], label = 'loss')\n","plt.plot(r.history['val_loss'], label = 'loss')\n","plt.legend()\n","plt.show()\n","plt.plot(r.history['accuracy'], label = 'accuracy')\n","plt.plot(r.history['val_accuracy'], label = 'val_accuracy')\n","plt.legend()\n","plt.show()\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Train on 60000 samples, validate on 10000 samples\n","Epoch 1/50\n","60000/60000 [==============================] - 7s 113us/sample - loss: 0.5201 - accuracy: 0.8135 - val_loss: 0.4233 - val_accuracy: 0.8447\n","Epoch 2/50\n","60000/60000 [==============================] - 6s 105us/sample - loss: 0.3985 - accuracy: 0.8541 - val_loss: 0.3838 - val_accuracy: 0.8592\n","Epoch 3/50\n","60000/60000 [==============================] - 6s 101us/sample - loss: 0.3643 - accuracy: 0.8662 - val_loss: 0.3832 - val_accuracy: 0.8610\n","Epoch 4/50\n","60000/60000 [==============================] - 6s 100us/sample - loss: 0.3428 - accuracy: 0.8734 - val_loss: 0.3735 - val_accuracy: 0.8673\n","Epoch 5/50\n","60000/60000 [==============================] - 6s 100us/sample - loss: 0.3279 - accuracy: 0.8777 - val_loss: 0.3682 - val_accuracy: 0.8661\n","Epoch 6/50\n","60000/60000 [==============================] - 6s 102us/sample - loss: 0.3154 - accuracy: 0.8820 - val_loss: 0.3499 - val_accuracy: 0.8762\n","Epoch 7/50\n","60000/60000 [==============================] - 6s 98us/sample - loss: 0.3045 - accuracy: 0.8862 - val_loss: 0.3369 - val_accuracy: 0.8830\n","Epoch 8/50\n","60000/60000 [==============================] - 6s 95us/sample - loss: 0.2945 - accuracy: 0.8893 - val_loss: 0.3330 - val_accuracy: 0.8758\n","Epoch 9/50\n","60000/60000 [==============================] - 6s 98us/sample - loss: 0.2870 - accuracy: 0.8908 - val_loss: 0.3347 - val_accuracy: 0.8774\n","Epoch 10/50\n","60000/60000 [==============================] - 6s 98us/sample - loss: 0.2793 - accuracy: 0.8952 - val_loss: 0.3325 - val_accuracy: 0.8830\n","Epoch 11/50\n","60000/60000 [==============================] - 6s 98us/sample - loss: 0.2715 - accuracy: 0.8985 - val_loss: 0.3278 - val_accuracy: 0.8853\n","Epoch 12/50\n","60000/60000 [==============================] - 6s 98us/sample - loss: 0.2663 - accuracy: 0.8999 - val_loss: 0.3411 - val_accuracy: 0.8830\n","Epoch 13/50\n","60000/60000 [==============================] - 6s 95us/sample - loss: 0.2629 - accuracy: 0.9013 - val_loss: 0.3229 - val_accuracy: 0.8871\n","Epoch 14/50\n","60000/60000 [==============================] - 6s 96us/sample - loss: 0.2547 - accuracy: 0.9049 - val_loss: 0.3346 - val_accuracy: 0.8838\n","Epoch 15/50\n","60000/60000 [==============================] - 6s 98us/sample - loss: 0.2525 - accuracy: 0.9041 - val_loss: 0.3186 - val_accuracy: 0.8922\n","Epoch 16/50\n","60000/60000 [==============================] - 6s 99us/sample - loss: 0.2445 - accuracy: 0.9071 - val_loss: 0.3261 - val_accuracy: 0.8885\n","Epoch 17/50\n","60000/60000 [==============================] - 6s 97us/sample - loss: 0.2434 - accuracy: 0.9067 - val_loss: 0.3315 - val_accuracy: 0.8902\n","Epoch 18/50\n","60000/60000 [==============================] - 6s 95us/sample - loss: 0.2353 - accuracy: 0.9110 - val_loss: 0.3484 - val_accuracy: 0.8848\n","Epoch 19/50\n","52448/60000 [=========================>....] - ETA: 0s - loss: 0.2348 - accuracy: 0.9112"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ddm37c530D7v","colab_type":"code","colab":{}},"source":["# evaluate\n","print('Model score:', model.evaluate(x_test, y_test))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9Z2lANIu0SHE","colab_type":"code","colab":{}},"source":["# take the predictions\n","y_pred = model.predict(x_test).argmax(axis = 1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ji1zLp4L0UV0","colab_type":"code","colab":{}},"source":["# look at an example of misclassification\n","\"\"\"\n","LABELS:\n","0\tT-shirt/top\n","1\tTrouser\n","2\tPullover\n","3\tDress\n","4\tCoat\n","5\tSandal\n","6\tShirt\n","7\tSneaker\n","8\tBag\n","9\tAnkle boot\n","\"\"\"\n","misclassified_idx = np.where(y_pred != y_test)[0]\n","e = np.random.choice(misclassified_idx)\n","plt.imshow(x_test[e], cmap = 'gray')\n","plt.title('True label : {}, Predicted Label : {}'.format(y_test[e], y_pred[e]))"],"execution_count":0,"outputs":[]}]}