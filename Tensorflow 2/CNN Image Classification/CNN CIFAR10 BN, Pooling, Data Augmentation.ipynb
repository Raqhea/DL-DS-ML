{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CNN CIFAR10 BN, Pooling, Data Augmentation-DO NOT PUSH.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMqOL0B8LQ3StDk17t+3bU9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"-kb-uIqAr8sa","colab_type":"code","outputId":"ea0ca653-b60c-490a-e7b9-bc4e70e50dac","executionInfo":{"status":"ok","timestamp":1583017544626,"user_tz":-180,"elapsed":11822,"user":{"displayName":"Alper Balbay","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjf7mL8J_VkRjlogmAa7HYBCvaLMJSd7yIl5KJK=s64","userId":"00418095213724128690"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["%tensorflow_version 2.x\n","import tensorflow as tf\n","\n","# layers\n","from tensorflow.keras.layers import Input, Dense, Dropout, Conv2D, MaxPooling2D, Flatten, BatchNormalization\n","# model\n","from tensorflow.keras.models import Model\n","\n","# dataset\n","from tensorflow.keras.datasets import cifar10"],"execution_count":0,"outputs":[{"output_type":"stream","text":["TensorFlow 2.x selected.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"HM4mmo_usmTA","colab_type":"code","colab":{}},"source":["# additional imports\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZoxzaFoCsqX9","colab_type":"code","outputId":"f8cd87cc-b012-4982-8f1d-d39ece0cc8d0","executionInfo":{"status":"ok","timestamp":1583017553353,"user_tz":-180,"elapsed":20529,"user":{"displayName":"Alper Balbay","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjf7mL8J_VkRjlogmAa7HYBCvaLMJSd7yIl5KJK=s64","userId":"00418095213724128690"}},"colab":{"base_uri":"https://localhost:8080/","height":119}},"source":["# load the data\n","cifar10_dataset = cifar10.load_data() # this'll return a tuple, first element contains training data and the second one contains test data\n","(x_train, y_train), (x_test, y_test) = cifar10_dataset\n","\n","# look at the shape of the data\n","print('Shape of the X: \\n Test: {}, Train: {}'.format(x_test.shape, x_train.shape))\n","print('Shape of the Y: \\n Test: {}, Train: {}'.format(y_test.shape, y_train.shape))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n","170500096/170498071 [==============================] - 6s 0us/step\n","Shape of the X: \n"," Test: (10000, 32, 32, 3), Train: (50000, 32, 32, 3)\n","Shape of the Y: \n"," Test: (10000, 1), Train: (50000, 1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"oSl7dAf1s_Wu","colab_type":"code","outputId":"7643bda0-2525-4d57-f53d-6217e47248fa","executionInfo":{"status":"ok","timestamp":1583017553354,"user_tz":-180,"elapsed":20523,"user":{"displayName":"Alper Balbay","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjf7mL8J_VkRjlogmAa7HYBCvaLMJSd7yIl5KJK=s64","userId":"00418095213724128690"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["# we don't need to process the X because its already 3D.\n","# but we need to flatten the Y\n","y_train, y_test = y_train.flatten(), y_test.flatten()\n","print('Shape of the Y: \\n Test: {}, Train: {}'.format(y_test.shape, y_train.shape))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Shape of the Y: \n"," Test: (10000,), Train: (50000,)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4zwB2de3tSu1","colab_type":"code","outputId":"77a7b4e8-9e10-4bb6-ef1e-5c44a48e051a","executionInfo":{"status":"ok","timestamp":1583017555365,"user_tz":-180,"elapsed":22526,"user":{"displayName":"Alper Balbay","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjf7mL8J_VkRjlogmAa7HYBCvaLMJSd7yIl5KJK=s64","userId":"00418095213724128690"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["# X contains the pixel values of images, these values are in the range of 0,255\n","# i'd like to centerize my data if i can because lower variance means more uniformly distributed data. So i'll divide all the values by 255, this will reduce the range to 0,1\n","print(x_train.std(), x_train.mean())\n","x_train, x_test = x_train / 255.0, x_test / 255.0\n","print(x_train.std(), x_train.mean())"],"execution_count":0,"outputs":[{"output_type":"stream","text":["64.1500758911213 120.70756512369792\n","0.25156892506322026 0.4733630004850874\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WOEMt--QuibY","colab_type":"code","outputId":"76c7a8bb-3047-4042-90d7-ae58d64eaae1","executionInfo":{"status":"ok","timestamp":1583017555366,"user_tz":-180,"elapsed":22517,"user":{"displayName":"Alper Balbay","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjf7mL8J_VkRjlogmAa7HYBCvaLMJSd7yIl5KJK=s64","userId":"00418095213724128690"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["x_train[0].shape"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(32, 32, 3)"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"rrSOnRqRuPNI","colab_type":"code","outputId":"fa6bd24a-47d4-4cd7-d6c0-6422c71e615b","executionInfo":{"status":"error","timestamp":1583052811991,"user_tz":-180,"elapsed":1661,"user":{"displayName":"Alper Balbay","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjf7mL8J_VkRjlogmAa7HYBCvaLMJSd7yIl5KJK=s64","userId":"00418095213724128690"}},"colab":{"base_uri":"https://localhost:8080/","height":231}},"source":["# Now we can build the model using functional api\n","i = Input(shape = x_train[0].shape)\n","x = Conv2D(32, (3,3), activation = 'relu', padding = 'same')(i)\n","x = BatchNormalization()(x)\n","x = Conv2D(32, (3,3), activation = 'relu', padding = 'same')(x)\n","x = BatchNormalization()(x)\n","x = MaxPooling2D((2,2))(x)\n","\n","x = Conv2D(64, (3,3), activation = 'relu', padding = 'same')(x)\n","x = BatchNormalization()(x)\n","x = Conv2D(64, (3,3), activation = 'relu', padding = 'same')(x)\n","x = BatchNormalization()(x)\n","x = MaxPooling2D((2,2))(x)\n","\n","x = Conv2D(128, (3,3), activation = 'relu', padding = 'same')(x)\n","x = BatchNormalization()(x)\n","x = Conv2D(128, (3,3), activation = 'relu', padding = 'same')(x)\n","x = BatchNormalization()(x)\n","x = MaxPooling2D((2,2))(x)\n","\n","x = Flatten()(x) # Dense layer'ı input olarak 1D array alır\n","x = Dropout(.4)(x) # to reduce overfitting\n","x = Dense(512, activation = 'relu')(x)\n","x = Dropout(.2)(x) # to reduce overfitting\n","x = Dense(10, activation = 'softmax')(x)\n","\n","model = Model(i, x)"],"execution_count":0,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-b4ef392b3df9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'same'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchNormalization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'same'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchNormalization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'Input' is not defined"]}]},{"cell_type":"code","metadata":{"id":"07XnREbcx1E-","colab_type":"code","colab":{}},"source":["# compile the model\n","model.compile(optimizer = 'adam',\n","              loss = 'sparse_categorical_crossentropy',\n","              metrics = ['accuracy'])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"c7jAMYzrx-QW","colab_type":"code","colab":{}},"source":["# fit the model\n","r = model.fit(x_train, y_train, validation_data = (x_test, y_test), epochs = 50)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Lxe06LgH7Tar","colab_type":"code","colab":{}},"source":["# data augmention\n","batch_size = 32\n","data_generator = tf.keras.preprocessing.image.ImageDataGenerator(width_shift_range=.1, height_shift_range = .1, horizontal_flip = True)\n","train_generator = data_generator.flow(x_train, y_train, batch_size)\n","steps_per_epoch = x_train.shape[0] // batch_size\n","r = model.fit_generator(train_generator, validation_data = (x_test, y_test), steps_per_epoch=steps_per_epoch, epochs = 50)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WDk35-DiyPbW","colab_type":"code","colab":{}},"source":["# plot the results\n","\n","# loss\n","plt.plot(r.history['loss'], label = 'loss')\n","plt.plot(r.history['val_loss'], label = 'val_loss')\n","plt.legend()\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zGj-ZPwHy3ZO","colab_type":"code","colab":{}},"source":["# accuracy\n","plt.plot(r.history['acc'], label = 'accuracy')\n","plt.plot(r.history['val_acc'], label = 'val_accuracy')\n","plt.legend()\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yOYghGbHy70u","colab_type":"code","colab":{}},"source":["from sklearn.metrics import confusion_matrix\n","import itertools\n","\n","def plot_confusion_matrix(cm, classes,\n","                          normalize = False,\n","                          title = 'Confusion Matrix',\n","                          cmap = plt.cm.Blues):\n","  \"\"\"\n","  This function prints and plots the confusion matrix.\n","  Normalization can be applied by setting 'normalize=True'.\n","  \"\"\"\n","  if normalize:\n","    cm = cm.astype('float') / cm.sum(axis = 1)[:, np.newaxis]\n","    print('Normalized confusion matrix')\n","  else:\n","    print('Confusion matrix, without normalization')\n","  print(cm)\n","\n","  plt.imshow(cm, interpolation = 'nearest', cmap = cmap)\n","  plt.title(title)\n","  plt.colorbar()\n","  tick_marks = np.arange(len(classes))\n","  plt.xticks(tick_marks, classes, rotation = 45)\n","  plt.yticks(tick_marks, classes)\n","\n","  fmt = '.2f' if normalize else 'd'\n","  thresh = cm.max() / 2.\n","  for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n","    plt.text(j, i, format(cm[i, j], fmt),\n","             horizontalalignment='center',\n","             color = 'white' if cm[i, j] > thresh else 'black')\n","  plt.tight_layout()\n","  plt.ylabel('True Label')\n","  plt.xlabel('Predicted Label')\n","  plt.show()\n","\n","p_test = model.predict(x_test).argmax(axis = 1)\n","cm = confusion_matrix(y_test, p_test)\n","plot_confusion_matrix(cm, list(range(10)))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zbsYaERgzWe3","colab_type":"code","colab":{}},"source":["# summary of model\n","model.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"99zUc-uizZ8W","colab_type":"code","colab":{}},"source":["labels = ['uçak', 'otomobil', 'kuş', 'kedi', 'geyik', 'köpek', 'kurbağa', 'at', 'gemi', 'kamyon']\n","# random false predictions\n","misclsf_idx = np.where(p_test != y_test)[0]\n","random_element = np.random.choice(misclsf_idx)\n","plt.imshow(x_test[random_element])\n","plt.title('Tahmin edilen: {} | Gerçekte olan: {}'.format(labels[p_test[random_element]], labels[y_test[random_element]]))"],"execution_count":0,"outputs":[]}]}